---
output:
  word_document: default
  pdf_document: default
---

# Discussion

A growing body of work assessing relationships at the day level is producing valuable advances to our understanding of workplace relationships. This research applies sophisticated methodology to bolster confidence in its findings. Unfortunately, however, attention is paid to just that: the statistics/methods and not the assumptions underlying inference. Our results show that substantially inflated Type I errors occur even when statistical attempts, such as partialling out baseline effects, are used as safeguards in HLM. This means that inferences about directionality need to be paired with assumptions about the world, because models produce the same result across a variety of different worlds (i.e., simulation environments). Despite a significant finding, therefore, we cannot be sure which world we are in. 

Our results also show that modeling strategies miss the effect (i.e., Type II errors) when assumptions about the lag structure imperfectly align with the true process. The implication of this is that minute differences between the true timing of events and those implied by the model can hinder inference. Discussions about how the window of measurement connects to the causal structure of the process in longitudinal designs exist [e.g., @Mathieu2006; @Miner2006] but are infrequent. They will need to develop alongside empirical evidence concerning the duration and lag structure of effects [@Bergh1993; @Grant2009] to provide indices for modeling in longitudinal work. 

Finally, the results speak to the compounding difficulties in longitudinal investigations. Stated simply, there are more aspects to be wrong about. Misspecified lag effects can occur even if the model direction is correct, and tailoring bias when the model direction is incorrect requires perfect modeling of the lag structure. Given the current ambiguity of lag effects these challenges are worrisome for longitudinal modelers. We recommend research on this area begin immediately. 

In the context of the methods discussed here, inferences concerning directionality are therefore tied to the following assumptions: (1) the relationship works in the direction stated by the researcher, (2) if the relationship exists in the reverse direction the lagged effects are captured by the model (i.e., there are no lagged effects unaccounted for), (3) the universe of variables not captured by the model that influence the predictors are independent of those that influence the outcome, and (4) the lag structure implied by the sampling and modeling strategies align with the underlying process. Although other assumptions are also at play [e.g., $y$ is a linear function of $x$ and it applies to all individuals; @Wainer1991], those stated represent the largest missing pieces in current research.

Notice that our paper follows the same form as the example presented from @Simon1954. There, multiple worlds were possible given a statistical result. The statistical result was a correlation coefficient reduction and the potential worlds were “spurious relationship” or “intervening variable.” The former was chosen based on an assumption of independence. Here, our statistical result was a significant beta weight and our interpretations could be any one of the three simulation worlds. Which we choose, therefore, depends on assumptions we are willing to make. 

Our discussion of different “worlds” is similar to the large and small world analogy used by @McElreath2016. He describes the small world as our modeling efforts. Although we attempt to use small world information to describe the large world (truth), we can never be sure if there are importance pieces in the large world that we are missing. Both of our discussions point to the limitations of focusing only on modeling; we prefer to use a “multiple world” analogy, rather than a “small vs. large world” analogy, because the latter makes it appear as though modeling techniques at least provide some information about the large world. In other words, the statistical inference is correct but incomplete. To the contrary, we wish to highlight that we could be in a world completely different from the one we think we are in.

## Theoretical Considerations

Management research tests models generated from theories [@Williams2003] that are inherently causal [@Aguinis2014] to make practical recommendations based on the findings [@Williams1995]. Some may argue that the theory discussed in an article’s introduction section provides the assumptions necessary to guide inference. We would agree if theories in organizational science were described in mathematical language [@Pearl2009] or specific enough to facilitate translation into a model. Yet, this practice is uncommon in our literature [@Weinhardt2012; @Ilgen2000] and we lack theoretical frameworks that specify time intervals, duration, or how processes fluctuate across time [@Miner2006; @Grant2009]. Others have also noted that our theories are vague [@Edwards2010; @Leavitt2010; @Cortina2017] and characterized by “creative language” [@Cucina2016; @Cortina2016; @Hambrick2007; @Mathieu2016], which means many statistical models can be implemented to test them. Incorporating assumptions – specific to the statistical model applied – helps identify the restrictions made when translating a theoretical model to a statistical one. 

Examples of the difficulty of translating theory into models are not hard to find. Both @Pitariu2010 and @Miner2006 noted that studies incorporating dynamic theories into their research propose and test static, rather than dynamic, hypotheses and models. It is also common for authors to specify structural equation models that are inconsistent with their proposed theory [@Oboyle2011; @Cortina2017]. Despite the importance of theory to our field [@Dubin1976; @Whetten1989; @George2000; @Mitchell2001] therefore, representing it correctly in a statistical model is a challenge. 

Our translation ability may also be stifled by a variety of factors that narrow our ability to think of alternatives. First, testing other causal pathways in empirical studies is challenging because of the multitude of alternative explanations [@Aguinis2014], which may create “helplessness” when considering other possibilities. Second, our field tends to focus on mid-range theories [@Landy1987] that explicitly exclude other possibilities to help characterize specific situations. Moreover, many of our theories agree on their core tenants [@Platt1964], and therefore reduce the number of alternative explanations we consider. All of these aspects limit our consideration of other possible worlds. We wish to highlight, however, that these are conjectures. Other areas of our field do focus on alternative explanations [e.g., divergent validity, @Schmitt1978] and exploring alternatives is a foundational scientific endeavor [@Deutsch2011]. The point here is that exploring alternatives is difficult. 

Theoretical discussions in the ESM studies reviewed are grounded in affective events [@Weiss1996], emotional labor [@Grandey2000] and ego-depletion [@Baumeister2000] theories that emphasize how individuals respond to a large span of factors in their environment. Many different sources and directions of influence are then possible. Discriminating these effects only becomes more difficult when analyzed over time because the individual interacts with increasingly more factors, which raises the number of possible outcome trajectories [@Einstein1911; @Heisenberg1927]. Variables in these studies are affective (e.g., positive affect), behavioral (e.g., helping behaviors), attitudinal (e.g., satisfaction), and visceral (e.g., depletion) that, unlike the intervening variable cases described at the beginning of the paper, do not have obvious directional effects. Assumptions are then necessary to develop conclusions.

## Implications

There are immediate and broad implications of our paper. Chewing on worldly assumptions during research investigations are the immediate implications. Statistical routines do a poor job of approximating causation because different types of data-generating processes (i.e., causation) result in the same data. Partialling a lagged variable, therefore, cannot establish causal priority [@Bollen1989]. We rarely, if ever, have access to the data generating process (another way to say that is all models are wrong) and should therefore use models to demonstrate our assumptions about the world rather than proofs of causation. Thinking hard about other possible worlds is critical in research, we therefore recommend questions about the direction, timing, duration, and structure of effects be placed at the forefront of any investigation.

Our demonstration also reveals several broader areas for useful research moving forward. As discussed, it will be necessary to develop empirical benchmarks of lagged effects. Establishing standards helps create a consistent paradigm necessary for the advancement of a scientific field because subsequent research can test the boundary conditions of these standards and move the field forward in a piecemeal fashion [@Kuhn1970]. In the context of longitudinal research, these standards would entail expectations regarding the lag structure of a given set of variables. For example, in the trier social stress paradigm [@Kirschbaum1993] there are clear, empirically established criteria for the timing of effects between a social stressor ($x$) and hormones ($y$). How much time $x$ takes to cause $y$, prior behavior of $y$, and the duration of the effect are all given considerable attention. In that paradigm, however, $x$ is a one-time manipulation, whereas longitudinal research in organization science is typically done in the field and observes non-manipulated independent variables that fluctuate as they please. All of the questions highlighted above, therefore, also need to be examined with respect to $x$ [@Edwards2008]. Clearly, creating benchmarks for the timing of effects to help inform research in field settings is a fruitful area for future laboratory research. The same could be said for laboratory work on directional effects, which would investigate whether reciprocal influence is possible [@Mook1983] and when different directions of influence are expected.

Empirical investigations of the type described above can then help develop stronger theory and non nil predictions. @Meehl1967, and other more recent authors [@Gigerenzer1998; @Cortina2011], suggest that the framework of our theories and tests limit meaningful research progress. They argue that psychological theories rarely predict anything beyond "$x$ influences $y$" and the null hypotheses we establish are straw arguments of "no effect." It then becomes superficially easy to make Type I errors or relate findings to a vague, overarching theory, resulting in a literature that is uninterpretable [@Meehl1990]. Strong theories predict a value, or range of values, that are clearly testable. Benchmarking effect timing, direction, and duration would promote theoretical explanations that are specific enough to make non nil predictions.

## Limitations

Several limitations are noteworthy. Our analysis did not explore connections between construct validity and modeling efforts. This was done to emphasize using models to capture the data-generating process. If measures are not valid indicators of the constructs involved in the underlying process any modeling application is futile. More research is needed to determine the severity of bias in modeling fidelity at various levels of construct validity.

Other statistical procedures than those investigated here are applicable to longitudinal data structures. Although we believe the general conclusion holds across modeling efforts, a similar presentation could be made across models by future work on this topic.
	
Finally, constraints were included in this paper to be consistent with the reviewed literature. Sample and effect sizes and the length of the process modeled were held constant. The magnitude of bias could therefore change in either direction under different conditions. We urge caution in generalizing the specific numbers reported here; the notion of worldly assumptions is the point to be taken.

# Conclusion

Despite their value, statistical routines cannot drive conclusions without underlying assumptions about the nature of the world. Our results show that both Type I and II errors occur when directional or lagged effects are misspecified, and including a lagged variable, unfortunately, does not provide the benefit of establishing relationship directionality. Various directions, lags, and causal structures are all assumed to be adequately represented in the model, but modeling techniques alone do not imply adequate representation of the data-generating process. Our results highlight important questions that merit consideration in any research investigation and point to fruitful areas for future research on the timing and direction of effects.
