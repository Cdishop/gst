---
output: pdf_document
---

Conducting a statistical analysis requires making assumptions, which can be distributional (e.g., normality of errors) or tied to a broader notion concerning the model’s fidelity in representing the world [@McElreath2016; @Bollen1989; @Granger2004]. Different data generating processes, which we refer to as “worlds,” can result in the same data and, once a model is applied, fit indices. Inferences, therefore, are rooted on assumptions about which world we believe we are in. The current paper emphasizes these “worldly” assumptions in light of three recent developments. 

First, increasing interest in process dynamics and longitudinal data structures [@Braun2013; @Bergh1995; @DeShon2012] highlights a need to revisit and revise the important causal inference debates that occurred in the context of cross-sectional data [@Cook1979; @Howson2006; @Freedman1999; @Holland1988; @Shadish2002]. Methodological researchers in organizational science advocate using repeated observations [@Ployhart2010; @Ployhart2008; @George2000; @Mitchell2001; @Aguinis2014] to reveal relationships not readily seen in cross-sectional research designs [e.g., @Molenaar2004; @Vancouver2006]. Substantive researchers in areas such as motivation, teams, affect, and multilevel theory [@Sonnentag2012; @Dalal2008; @Neal2017; @Kozlowski2006; @Kozlowski2000; @Weiss1996] also call for longitudinal studies to refine our understanding of organizational phenomena. As a result, a large literature on process dynamics [e.g., @Chun2014; @Mathieu2015; @Piening2013; @Sonnentag2008; @Wang2010] now exists and is expected to increase in both size and scope [@DeShon2012]. Moreover, recent public interest in self-tracking [@Nef2016], and the methodological and technological improvements that followed [@Braun2013], allow researchers to more readily collect longitudinal data than ever before. These developments helped event sampling methodology become one of the most common data collection techniques used in studies published by top organizational journals [Scandura & Williams -@Scandura2000].

Second, statistical techniques tend to receive more attention than deeper notions of causal inference in both methods publications [@Aguinis2014; @Mathieu2006] and graduate school training [@Aiken2008; @Tett2013]. Complex statistical procedures are required to handle longitudinal data [@Kuljanin2011], therefore “how-to” papers on the latest techniques are constantly emerging, such as Jebb, Tay, Wang, and Huang’s [-@Jebb2015] time series presentation; Pitariu and Ployhart’s [-@Pitariu2010] paper on dynamic mediation; Bliese and Ployhart’s [-@Bliese2002] discussion of growth modeling; DeShon’s [-@DeShon2012] chapter on multivariate dynamics; Schonfeld and Rindskopf’s [-@Schonfeld2007] application of hierarchical linear modeling to longitudinal data; Liu, Rovine, and Molenaar’s [-@Liu2012] comparison of different longitudinal modeling strategies; or Wang, Zhou, and Zhang’s [-@Wang2016] overview of dynamic modeling. These papers are unquestionably valuable but their prevalence may lead researchers to overweight the role of statistical techniques in the process of making scientific inferences. It is not surprising, then, that a growing number of event-sampling studies use a statistical approach to support an inference [@Scott2011; @Lanaj2014; @Lanaj2016; @Koopman2016; @Rosen2016; @Lanaj2016b; @Johnson2014; @Gabriel2017], namely, that controlling for a prior measurement of an outcome variable reduces concerns for reverse causation. Said another way, the notion is that partialling an outcome variable baseline observation provides evidence to support directionality inferences in longitudinal data structures. 

We investigate the extent to which directionality inferences are supported by using models that partial prior observations with the goal of reigniting broader causal discussions in the context of longitudinal data structures. Statistical routines alone do not mitigate concerns of reverse causation, rather, worldly assumptions posed by the researcher lead to a directionality inference. We hope accounting for lagged effects continues, but doing so must be paired with an appreciation of assumptions because the models themselves cannot, as we show in this paper, support an inference of directionality. 

We begin by highlighting some of the key assumptions behind directionality inferences (i.e., causality). After explicating these concepts, we simulate different “true worlds” and then apply the statistical techniques used in the aforementioned studies to illustrate the limitations of statistical models for establishing the nature of the world under investigation. 

Finally, our intent is to use this specific directionality case to call increased attention to the conditions needed to support causal inferences in longitudinal analysis. These discussions are necessary when initial enthusiasm surrounding the ability of a new statistical technique to support causal inference is curtailed and corrected as research develops. For instance, passive measurements of individual differences were once partialled out of multiple regression equations as a substitute for random assignment [@Cook1979], and intense debate on the ability of models to capture cause was needed following the emergence of structural equation modeling [@Brannick1995; @Williams1995] and directed acyclic graph models [@Dawid2010]. 

## Inference Requires Assumptions

Inferences regarding relationships between variables are based on (untestable) assumptions rather than statistics [@Binning1989; @Pearl2009; @McElreath2016; @Kuhn1970; @Freedman1991; @Dawid2016]. A clear demonstration of the role of assumptions is found in Simon’s (1954) presentation of the third variable problem. Suppose that a researcher observes a relationship between marital status ($x$) and candy consumption ($y$). If this relationship decreases considerably when a third variable, say age ($z$), is added to a statistical model then two conclusions are possible: a relationship exists between marital status ($x$) and candy consumption ($y$) but age ($z$) is an intervening variable; or the relationship between marital status ($x$) and candy consumption ($y$) is spurious because age ($z$) causes variation in both. We can conclude the second inference (a spurious association) if we are willing to make an assumption regarding the independence of age ($z$). That is, “apart from any statistical evidence, we are prepared to assert that the age of the person does not depend on either his candy consumption or his marital status” [@Simon1954, p. 471]. In this example two different worlds, an intervening variable world and a spurious relationship world, produce the same result after applying a statistical routine (e.g., partial correlation). One may suggest that a particular world is more likely than another but, ultimately, this reduces to an untested and possibly untestable assumption.

Assumptions often go unnoticed because of our shared intuitions about the research context. For example, Williams and Hazer [-@Williams1986] found that the relationship between work characteristics (e.g., the amount of perceived repetition) and turnover reduced when attitudes, such as job satisfaction and commitment, were included in the model. Attitudes could therefore be an intervening variable or a common cause. We agree with the authors’ conclusion that attitudes should be conceived as an intervening variable, but notice the underappreciated intuition we all share in this case: attitudes cannot cause work characteristics. Said another way, it is challenging to imagine a world in which an individual’s attitude produces the amount of repetition in their job. Because this intuition is obvious in many investigations the assumption goes unstated. An unintended consequence, however, is that it reduces the perceived need to highlight alternative explanations (e.g., different worlds) in studies with more ambiguous relationship directionality.

Assumptions about the nature of causation commonly arise in the context of group comparisons. Not every situation can be observed nor every group compared. Moreover, individuals cannot be tested in multiple groups at the same time. We are forced, then, to make counterfactual assumptions about what “would have happened” if individuals were placed into a different experimental group or tested at a different time [@Wainer2006]. This is the case even if these various groups and times are not used in the research design. For example, in a repeated measures design where a stimulus changes an outcome variable from its baseline we assume that, in an unassigned control condition where no stimulus is presented, scores on the outcome variable would be the same as the baseline set-point across time. Similarly, if scores from a treatment group are compared to an independent control group (i.e., a between subjects design), we must assume that individuals from the treatment group would have had similar scores to the control group had they been in the control group rather than the treatment group [@McGrath1981].

In statistical modeling, assumptions concern the nature of the data generating process. Although knowing whether or not we have the correct cause and all relevant variables are fundamental problems across all stages of scientific inference [@Howson2006], these issues are especially important in modeling because many different data generating processes can result in the same observed data [@McElreath2016]. Statistical models, therefore, cannot identify the causal process without relying on assumptions [@Pearl2012; @Pearl2009] because model fit and parameter estimates may be equivalent across different data generating worlds [@Mathieu2008]. The “answers” models provide are conditional on their own rules for interpreting data [@Granger2004] and every model has only one way of viewing the problem [@McElreath2016]. Bias can then arise when all of the important information is not included in the model [e.g., the omitted variables problem; @Anderson1992; @Williams1986; @Wooldridge2010]. Evaluating fit and parameter estimates indicates the discrepancy between the model and data, but it does not reveal the discrepancy between the model and reality [@Bollen1989]. This is not to say that causal inferences should be avoided when using models; we share Pearl’s view that they should be emphasized, but only when they are paired with explicit assumptions. 

## Assumptions in Longitudinal Research

We now shift to the specific assumptions embedded in longitudinal research. Questions about process concern the causal ordering and duration of development and change across variables. For example, @Mathieu2006 examined changes in work structures to improve employee moral and customer satisfaction. An explanation of this process addresses questions such as “how long does it take to establish the new work design? If employees are indeed more motivated to perform, how long will it take for customers to notice and for them to become more satisfied?” (p. 1035). The former is a question about stimulus length while the latter focuses on the lag-time between variables, or the time it takes for an effect to occur after a cause. Even before addressing these focal questions we must consider the causal ordering and number of variables in the system. If we knew the data generating process we could be confident in our ability to answer these questions. Since we do not, we make assumptions about causal order, duration of effects, the key variables, and the lag-length between them. All of these assumptions are implicitly embedded in the model or models we use to represent the patterns in the data. 

Assumptions also underlie sampling and measurement strategies. Measurement frequency and duration have to align with the data generating process to reveal it. Consider a process that oscillates as a sine wave over four iterations. An infrequent sampling scheme may reveal an increasing, decreasing, or stable trend depending on the sampling window, whereas frequent measurements consistent with the patterns of the underlying sine process reveal the fluctuations. Although frequency is a benefit in our example, of more importance is the ability of the sampling strategy to capture the underlying process. Sampling strategies (measurement frequency and duration) can therefore be viewed as instantiations of assumptions about the causal process dynamics.

Beyond these embedded assumptions there are also areas of ambiguity regarding sampling strategies in longitudinal research. For example, @Aguinis2014 advocate allowing the hypothesized cause to precede the effect to establish “stronger causal inferences” (p. 152). Conversely, @Bollen1989 suggests studying causes and effects with a minimal time gap in longitudinal research to reduce any confounding influence on the outcome variable. Such differences, paired with minimal empirical evidence to suggest the duration of certain effects [@Grant2009] and the array of sampling strategies available to researchers [e.g., time, event, or signal contingent; @Beal2003], leave ambiguity surrounding the frequency and duration of measurements needed to support causal inference.

Once data are collected, a wide variety of models and estimation strategies are used to represent the underlying process. Assumptions at this step concern data distributions and, as discussed earlier, model fidelity. The former, as elegantly summarized by @Bergh1993, constrain expectations about how data are dispersed both between units and over time. Trends in a system of time series trajectories can produce significant results even if all paths are independent [@Kuljanin2011], therefore data must be stationary or a co-integration technique is required [@Engle1987]. Whether effect estimates are allowed to vary creates assumptions about the stability of effects over time. For example, constraining the estimate of one effect to be equal across time assumes the effect of $x_{t}$ on $y_{t}$ is the same as the effect of $x_{t-1}$ on $y_{t-1}$ indefinitely. The analytics also need to account for autocorrelation [@Granger1974] and heteroscedasticity if pooling is applied [@Bergh1993]. These statistical issues are important in longitudinal analyses but they also receive disproportionate attention in the methods literature. Our emphasis is on model fidelity.

Model fidelity concerns the discrepancy between a model’s representation of the world and the actual world. Assumptions at this step are represented in various elements of how the model is formatted. Which variables are viewed as predictors, outcomes, and covariates? How many outcomes does each predictor influence? How long are the lags, should lags be included? Is the outcome generated by a linear function? These implicit questions manifest when modeling choices are made and reflect assumptions about the world. As noted earlier, techniques can reduce model-data differences, but capturing reality is much more difficult. For example, @Bollen1989 and @vonEye2012 demonstrated mathematically that lagging variables and regression techniques, respectively, cannot establish causal priority. Despite this, there is confusion in the event sampling literature, where a baseline of the dependent variable is controlled in a statistical model to “diminish concerns of reverse causality” [@Lanaj2016b, p.1102]. The estimation technique used (HLM) requires distributional assumptions that we feel are well accounted for by researchers. Other worldly assumptions, however, are also needed, including that (1) the process unfolds in the proposed direction (2) by a linear function (3) with no other variables influencing the system, (4) a lag structure that aligns with the sampling and modeling strategy, and (5) lagged effects that are captured by the model if the relationship exists in the reverse direction. In other words, we have to assume the nature of the world that the model is trying to describe.

Assumptions concerning the structure of the process under investigation are always needed, especially in the context of observational research. Having discussed the need for assumptions in inference we now shift to empirical demonstrations of these issues.  We create a variety of simulation worlds and apply models consistent with the event sampling literature to illustrate their limitation in establishing causal direction. We first describe the typical design and estimation methods used in the ESM studies. Then, Monte Carlo simulations highlight that the same findings can emerge despite vastly different “true worlds.”

## ESM Data and Models

ESM studies typically collect responses over a two week period. This observation window follows from Wheeler's [-@Wheeler1991] suggestion that this timeframe represents a generalizable sample of individual human behavior. Typically, one to three measurements are taken per day at various locations. For example, an employee may answer a short survey when they wake up in the morning, another during their lunch break, and a final survey at the end of their day either on-site or at their home. 

A common set of sampling and modeling techniques are then used. First, the outcome of interest is measured twice per day. For example, affect ($y$) is measured in the morning and afternoon [@Scott2011; @Koopman2016; @Lanaj2016] while the independent variable ($x$) is measured only in the afternoon. A model is then applied to the data such that the outcome variable ($y$) is regressed onto one or more predictor variables ($x$) while also including the outcome variable from the previous time point ($y_{t-1}$). This generic model may be represented as:
\begin{equation}
y_{it}= b_0 + b_1 y_{i{(t-1)}} + b_2 x_{it} + e_{it}
\end{equation}
\noindent where $y_{it}$ is the value of the outcome for the $i^{th}$ person at time $t$, $x_{it}$ is the value of the predictor variable for the $i^{th}$ person at time $t$, $y_{i(t-1)}$ is the value of the outcome for the $i^{th}$ person at time $t-1$, $e_{it}$ is a normally distributed error term for the $i^{th}$ person at time $t$, and $b_0$, $b_1$, and $b_2$ represent the regression intercept and partial regression coefficients relating the predictors to the outcome.

The natural nesting of observations within units in ESM studies requires accounting for clustering when estimating the model parameters. Hierarchical linear modeling (HLM), also known as multi-level or random intercepts modeling, is the typical estimation technique used in ESM research when accounting for clustering is important. Many excellent overviews of this modelling approach exist already [e.g., @Finch2014; @Schonfeld2007], so we only cover what is necessary for the present purposes. HLM accounts for clustering or unit effects by incorporating random effects that may vary across units. HLM is capable of accounting for many forms of clustering but, typically, ESM studies use an approach allowing intercepts to vary across units and, therefore, assume that the random intercepts follow a normal distribution. The generic model presented above may be generalized to represent this model by incorporating random intercepts such that:   
\begin{equation}
y_{it} = b_{0_i} + b_1 y_{i(t-1)} + b_2 x_{it} + e_{it}
\end{equation}
\noindent 
where all terms are defined as above except the intercept $b_{0_i}$ may now be unique to the $i^{th}$ person or unit and $b_{0_i}$ is assumed to be distributed $N(\gamma,\sigma^2_{b_0})$.
